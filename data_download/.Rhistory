site.df$n_days[i] <- NA
}
}
View(site.df)
# Filter sites with temperature data
temp_sites <- site.df[which(is.na(site.df$begin_date) == FALSE),]
View(temp_sites)
# Write sites with temperature data to csv
write.csv('temp_sites.csv')
# Write sites with temperature data to csv
write.csv(temp_sites,'temp_sites.csv')
# Create dataframe for identifying years with the most complete data
tempmat <- matrix(nrow=1468,ncol=37)
date4yr.df <- cbind(temp_sites,tempmat)
# Create dataframe for identifying years with the most complete data
tempmat <- matrix(nrow=dim(temp_sites)[2],ncol=37)
# Create dataframe for identifying years with the most complete data
tempmat <- matrix(nrow=dim(temp_sites)[1],ncol=37)
date4yr.df <- cbind(temp_sites,tempmat)
names(date4yr.df)[5:41] <- paste("Y",seq(1980,2016),"-",seq(1984,2020),sep="")
View(date4yr.df)
# Create list of intervals for start and end dates of years 1980-2020
int.list4yr <- list()
for (i in seq(1980,2016)){
# Define start and end dates
d1 <- as.Date(paste(i,"-01-01",sep=""),format="%Y-%m-%d")
d2 <- as.Date(paste((i+4),"-01-01",sep=""),format="%Y-%m-%d")
# Add interval to lsit
int.list4yr[[(i-1979)]] <- interval(d1,d2)
}
View(int.list4yr)
for (i in seq(1980,2016)){
# Define start and end dates
d1 <- as.Date(paste(i,"-01-01",sep=""),format="%Y-%m-%d")
d2 <- as.Date(paste((i+4),"-01-01",sep=""),format="%Y-%m-%d")
# Add interval to lsit
int.list4yr[[(i-1979)]] <- interval(d1,d2)
}
library(lubridate)
for (i in seq(1980,2016)){
# Define start and end dates
d1 <- as.Date(paste(i,"-01-01",sep=""),format="%Y-%m-%d")
d2 <- as.Date(paste((i+4),"-01-01",sep=""),format="%Y-%m-%d")
# Add interval to lsit
int.list4yr[[(i-1979)]] <- interval(d1,d2)
}
View(int.list4yr)
# Loop through sites to identify years with most data
for (i in seq(1, length(temp_sites$site))){
# Define start and end date interval
int <- interval(date4yr.df$begin_date[i], date4yr.df$end_date[i])
# Loop through interval list
for (j in seq(1,length(int.list4yr))){
# Check if interval is within each year's interval
if (int.list4yr[[j]] %within% int == TRUE) {
date4yr.df[i,(j+4)] <- 1 # Set year column to 1 if date is contained in range
} else {
date4yr.df[i,(j+4)] <- 0 # Set year column to 0 if date is contained in range
}
}
}
View(date4yr.df)
data_4yr <- colSums(date4yr.df[,c(5:41)])
data_4yr
# Pull out sites with data from 2016-2020
sites_16.20 <- date4yr.df[which(date4yr.df$`Y2016-2020` == 1),c(1:4)]
View(sites_16.20)
# Add apostrophe to site numbers to preserve leading 0 in Excel
sites_16.20$site <- paste("'",sites_16.20$site,sep="")
View(sites_16.20)
# Write sites to csv
write.csv(sites_16.20, "AllSites_with_2016_2020_data.csv",row.names=FALSE)
__DIR__
install.packages("rstudioapi")
library(rstudioapi)
library(rstudioapi)
dirname(getActiveDocumentContext()$path)
getActiveDocumentContext()
setwd("E:/Users/Jeff/Github/stream-temperature-rf/site_id")
getActiveDocumentContext()
dirname(getActiveDocumentContext()$path)
setwd(dirname(getActiveDocumentContext()$path))
rstudioapi::getActiveDocumentContext()$path
source("GAGES_StreamTemp_Sites.R", chdir = TRUE)
parent.frame(2)$ofile
dirname(rstudioapi::getActiveDocumentContext()$path)
dirname(rstudioapi::getActiveDocumentContext()$path)
library(lubridate)
library(dataRetrieval)
library(zoo)
# Read in site list (sites with any data during 2016-2020)
sites = read.csv("C:/Users/Jeff/StreamTemp/MonthlySlopes/Sites_2016_2020/SiteExploration/sitelist_2016_2020.csv")
# Read in site list (sites with any data during 2016-2020)
sites = read.csv("E:Users/Jeff/Github/stream-temperature-rf/site_id/AllSites_with_2016_2020_data.csv")
# Read in site list (sites with any data during 2016-2020)
sites = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/site_id/AllSites_with_2016_2020_data.csv")
# Remove apostrophe from sites
sites$site = sub('.','',sites$site)
# Set date range
startDate = "2016-10-01"
endDate = "2020-09-30"
# Set other USGS data retrieval parameters
parameterCd = "00010" # Temperature
#statCd = "00003" # Daily Mean
statCd = "00001" # Daily Max
# Create dataframe to store stream temperatures
temp.df = data.frame(Date = seq(from=ymd(startDate),to=ymd(endDate),by='days'))
temp.df$MonYr = paste(year(temp.df$Date),month(temp.df$Date),sep="")
# Add columns for each site
temp.df = cbind(temp.df,matrix(0,nrow=1461,ncol=412))
# Add columns for each site
temp.df = cbind(temp.df,matrix(0,nrow=1461,ncol=dim(sites)[1]))
# Create dataframe to store stream temperatures
temp.df = data.frame(Date = seq(from=ymd(startDate),to=ymd(endDate),by='days'))
temp.df$MonYr = paste(year(temp.df$Date),month(temp.df$Date),sep="")
# Add columns for each site
temp.df = cbind(temp.df,matrix(0,nrow=1461,ncol=dim(sites)[1]))
# Rename site columns
names(temp.df)[c(-1,-2)] = sites$ID
sites$ID
View(sites)
# Rename site columns
names(temp.df)[c(-1,-2)] = sites$site
View(temp.df)
# Create dummy zoo time series of dates for alignment
date.df = temp.df[,c(1,3)]
date.zoo = zoo(date.df[,-1],order.by = date.df[,1])
# Download data from each site
for (i in seq(1,length(sites$site))){
# Index site number
siteNumber = sites$site[i]
# Download data from site
tempdata = readNWISdv(siteNumber, parameterCd, startDate, endDate, statCd = statCd)
# Check if tempdata is empty
if (dim(tempdata)[1] != 0){
# Conver to zoo format
tempdata.zoo = zoo(tempdata[,4], order.by = tempdata[,3])
# Merge with dummy zoo time series to align dates
tempmerge = merge(date.zoo, tempdata.zoo, all= c(TRUE,TRUE), fill = NA)
# Store data in temp.df
temp.df[,(i+2)] = tempmerge$tempdata.zoo
}
}
View(temp.df)
maxsum = colSums(temp.df[,c(-1,-2)],na.rm=TRUE)
siteswithoutmax = which(maxsum == 0)
sitenameswithoutmax = names(siteswithoutmax)
siteswithoutmax
sitenameswithoutmax
# Subset disch.month by sites with complete data
temp.data = temp.df[,-which(names(temp.df) %in% sitenameswithoutmax)]
# Subset disch.month by sites with complete data
temp.df = temp.df[,-which(names(temp.df) %in% sitenameswithoutmax)]
# Find number of missing values at each site
na_count = sapply(temp.df[,c(-1,-2)], function(y) sum(length(which(is.na(y)))))
# Calculate percentage of missing values (x/1461)
na_percent = (na_count / 1461) * 100
# Sites with less than 10% of data missing
siteswithdata = na_percent[which(na_percent < 10)]
# Get names of sites with complete (>90%) data
site_names = names(siteswithdata)
# Subset temp.df using site_names
temp.df_withdata = temp.df[,c(1,2,match(site_names, names(temp.df)))]
View(temp.df_withdata)
View(temp.df_withdata)
write.csv(temp.df_withdata,"temp.df_withdata.csv")
# Read in site list (sites with any data during 2016-2020)
sites = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/site_id/AllSites_with_2016_2020_data.csv")
tempdata = readNWISdv(01187300, parameterCd, startDate, endDate, statCd = statCd)
tempdata = readNWISdv(01585230, parameterCd, startDate, endDate, statCd = statCd)
parameterCd = "00010" # Temperature
statCd = "00003" # Daily Mean
tempdata = readNWISdv(01585230, parameterCd, startDate, endDate, statCd = statCd)
tempdata = readNWISdv('01585230', parameterCd, startDate, endDate, statCd = statCd)
tempdata = readNWISdv('01187300', parameterCd, startDate, endDate, statCd = statCd)
# Read in site list (sites with any data during 2016-2020)
sites = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/site_id/AllSites_with_2016_2020_data.csv")
# Remove apostrophe from sites
sites$site = sub('.','',sites$site)
# Set date range
startDate = "2016-10-01"
endDate = "2020-09-30"
# Set other USGS data retrieval parameters
parameterCd = "00010" # Temperature
statCd = "00003" # Daily Mean
# Create dataframe to store stream temperatures
temp.df = data.frame(Date = seq(from=ymd(startDate),to=ymd(endDate),by='days'))
temp.df$MonYr = paste(year(temp.df$Date),month(temp.df$Date),sep="")
# Add columns for each site
temp.df = cbind(temp.df,matrix(0,nrow=1461,ncol=dim(sites)[1]))
# Rename site columns
names(temp.df)[c(-1,-2)] = sites$site
# Create dummy zoo time series of dates for alignment
date.df = temp.df[,c(1,3)]
date.zoo = zoo(date.df[,-1],order.by = date.df[,1])
# Download data from each site
for (i in seq(1,length(sites$site))){
# Index site number
siteNumber = sites$site[i]
# Download data from site
tempdata = readNWISdv(siteNumber, parameterCd, startDate, endDate, statCd = statCd)
# Check if tempdata is empty
if (dim(tempdata)[1] != 0){
# Conver to zoo format
tempdata.zoo = zoo(tempdata[,4], order.by = tempdata[,3])
# Merge with dummy zoo time series to align dates
tempmerge = merge(date.zoo, tempdata.zoo, all= c(TRUE,TRUE), fill = NA)
# Store data in temp.df
temp.df[,(i+2)] = tempmerge$tempdata.zoo
}
}
tempsum = colSums(temp.df[,c(-1,-2)],na.rm=TRUE)
siteswithouttemp = which(tempsum == 0)
sitenameswithouttemp = names(siteswithouttemp)
# Subset disch.month by sites with complete data
temp.df = temp.df[,-which(names(temp.df) %in% sitenameswithouttemp)]
# Find number of missing values at each site
na_count = sapply(temp.df[,c(-1,-2)], function(y) sum(length(which(is.na(y)))))
# Calculate percentage of missing values (x/1461)
na_percent = (na_count / 1461) * 100
# Sites with less than 10% of data missing
siteswithdata = na_percent[which(na_percent < 10)]
# Get names of sites with complete (>90%) data
site_names = names(siteswithdata)
setwd("E:/Users/Jeff/Github/stream-temperature-rf/data_download")
temp.df_withdata = temp.df[,c(1,2,match(site_names, names(temp.df)))]
# # Subset site information for sites with data
siteinfo = sites[match(site_names, sites$ID),c(2,1)]
siteinfo$site = paste("'",siteinfo$site,sep="")
write.csv(siteinfo, 'sitelist_2016_2020.csv',row.names=FALSE)
write.csv(temp.df_withdata,"temp.df_withdata.csv")
site_names
names(temp.df)
# # Subset site information for sites with data
siteinfo = data.frame(site=site_names)
siteinfo$site = paste("'",siteinfo$site,sep="")
View(siteinfo)
write.csv(siteinfo, 'sitelist_2016_2020.csv',row.names=FALSE)
siteinfo
View(siteinfo)
AT_17 = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/data_download/AirTemperature/PRISM_raw/PRISM_tmean_stable_4km_20161001_20170930.csv")
AT_18 = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/data_download/AirTemperature/PRISM_raw/PRISM_tmean_stable_4km_20171001_20180930.csv")
AT_19 = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/data_download/AirTemperature/PRISM_raw/PRISM_tmean_stable_4km_20181001_20190930.csv")
AT_20 = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/data_download/AirTemperature/PRISM_raw/PRISM_tmean_stable_4km_20191001_20200930.csv")
# Get list of site names
sites = unique(AT_17$Name)
View(AT_17)
View(AT_18)
AT_17 = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/data_download/AirTemperature/PRISM_raw/PRISM_tmean_stable_4km_20161001_20170930.csv")
AT_18 = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/data_download/AirTemperature/PRISM_raw/PRISM_tmean_stable_4km_20171001_20180930.csv")
AT_19 = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/data_download/AirTemperature/PRISM_raw/PRISM_tmean_stable_4km_20181001_20190930.csv")
AT_20 = read.csv("E:/Users/Jeff/Github/stream-temperatur
AT_20 = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/data_download/AirTemperature/PRISM_raw/PRISM_tmean_stable_4km_20191001_20200930.csv")
AT_20 = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/data_download/AirTemperature/PRISM_raw/PRISM_tmean_stable_4km_20191001_20200930.csv")
# Get list of site names
sites = unique(AT_17$Name)
# Remove blank value
sites = sites[-2]
# Create sequence of dates
dates = seq(from=ymd('2016-10-01'),to=ymd('2020-09-30'),by='days')
# Create dataframe to store air temperatures
at = data.frame(Date = dates, MonYr = paste(year(dates),month(dates),sep=""))
View(AT_17)
# Add columns for each site
at = cbind(at, matrix(nrow=1461,ncol=length(sites)))
# Rename columns by site name
names(at)[3:412] = sites
View(at)
# Loop through sites, storing values in at
for (i in seq(1,length(sites))){
# Index each year's CSV file, compiling data from each site
tempdata = c(AT_17$tmean_C[which(AT_17$Name == sites[i])],
AT_18$tmean_C[which(AT_18$Name == sites[i])],
AT_19$tmean_C[which(AT_19$Name == sites[i])],
AT_20$tmean_C[which(AT_20$Name == sites[i])])
# Store array in at
at[(i+2)] = tempdata
}
# Loop through sites, storing values in at
for (i in seq(1,length(sites))){
# Index each year's CSV file, compiling data from each site
tempdata = c(AT_17$tmean_F[which(AT_17$Name == sites[i])],
AT_18$tmean_F[which(AT_18$Name == sites[i])],
AT_19$tmean_F[which(AT_19$Name == sites[i])],
AT_20$tmean_F[which(AT_20$Name == sites[i])])
# Store array in at
at[(i+2)] = tempdata
}
View(at)
# Convert temperatures from F to C
at[,c(-1,-2)]
View(at)
# Convert temperatures from F to C
at[,c(-1,-2)] = 1.8*(at[,c(-1,-2)] - 32)
View(at)
AT_17 = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/data_download/AirTemperature/PRISM_raw/PRISM_tmean_stable_4km_20161001_20170930.csv")
AT_18 = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/data_download/AirTemperature/PRISM_raw/PRISM_tmean_stable_4km_20171001_20180930.csv")
AT_19 = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/data_download/AirTemperature/PRISM_raw/PRISM_tmean_stable_4km_20181001_20190930.csv")
AT_20 = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/data_download/AirTemperature/PRISM_raw/PRISM_tmean_stable_4km_20191001_20200930.csv")
# Convert temperatures to celcius
AT_17$tmean_C <- 1.8*(AT_17$tmean_F-32)
View(AT_17)
# Convert temperatures to celcius
AT_17$tmean_C <- (5/9)*(AT_17$tmean_F-32)
View(AT_17)
# Get list of site names
sites <- unique(AT_17$Name)
# Remove blank value
sites <- sites[-2]
# Create sequence of dates
dates <- seq(from=ymd('2016-10-01'),to=ymd('2020-09-30'),by='days')
# Create dataframe to store air temperatures
at <- data.frame(Date = dates, MonYr = paste(year(dates),month(dates),sep=""))
# Add columns for each site
at <- cbind(at, matrix(nrow=1461,ncol=length(sites)))
# Rename columns by site name
names(at)[3:412] <- sites
# Loop through sites, storing values in at
for (i in seq(1,length(sites))){
# Index each year's CSV file, compiling data from each site
tempdata <- c(AT_17$tmean_C[which(AT_17$Name == sites[i])],
AT_18$tmean_C[which(AT_18$Name == sites[i])],
AT_19$tmean_C[which(AT_19$Name == sites[i])],
AT_20$tmean_C[which(AT_20$Name == sites[i])])
# Store array in at
at[(i+2)] <- tempdata
}
View(AT_17)
AT_18$tmean_C <- (5/9)*(AT_18$tmean_F-32)
AT_19$tmean_C <- (5/9)*(AT_19$tmean_F-32)
AT_20$tmean_C <- (5/9)*(AT_20$tmean_F-32)
# Loop through sites, storing values in at
for (i in seq(1,length(sites))){
# Index each year's CSV file, compiling data from each site
tempdata <- c(AT_17$tmean_C[which(AT_17$Name == sites[i])],
AT_18$tmean_C[which(AT_18$Name == sites[i])],
AT_19$tmean_C[which(AT_19$Name == sites[i])],
AT_20$tmean_C[which(AT_20$Name == sites[i])])
# Store array in at
at[(i+2)] <- tempdata
}
View(at)
stmean = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/data_download/StreamTemperature/ST_2016_2020.csv")
View(stmean)
stmean = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/data_download/StreamTemperature/ST_2016_2020.csv")
View(stmean)
# Replace stream temperatures below 0 with 0 C.
# Loop through all sites
for (i in seq(3,412)){
# Replace values less than 0 with 0
stmean[which(stmean[,i] < 0),i] <- 0
}
which(stream < 0)
which(stmean < 0)
# Read in site list (sites with any data during 2016-2020)
sites = read.csv("E:/Users/Jeff/Github/stream-temperature-rf/data_download/StreamTemperature/sitelist_2016_2020.csv")
# Remove apostrophe from sites
sites$site = sub('.','',sites$site)
View(sites)
# Set date range
startDate = "2016-10-01"
endDate = "2020-09-30"
# Create dataframe to store dishcarge
disc.df = data.frame(Date = seq(from=ymd(startDate),to=ymd(endDate),by='days'))
disc.df$MonYr = paste(year(disc.df$Date),month(disc.df$Date),sep="")
# Add columns for each site
disc.df = cbind(disc.df,matrix(0,nrow=1461,ncol=453))
# Create dataframe to store dishcarge
disc.df = data.frame(Date = seq(from=ymd(startDate),to=ymd(endDate),by='days'))
disc.df$MonYr = paste(year(disc.df$Date),month(disc.df$Date),sep="")
# Add columns for each site
disc.df = cbind(disc.df,matrix(0,nrow=1461,ncol=410))
parameterCd = "00060" # Discharge
statCd = "00003" # Daily Mean
# Rename site columns
names(disc.df)[c(-1,-2)] = sites$ID
View(disc.df)
# Create dummy zoo time series of dates for alignment
date.df = disc.df[,c(1,3)]
date.zoo = zoo(date.df[,-1],order.by = date.df[,1])
# Download data from each site
for (i in seq(1,length(sites$site))){
# Index site number
siteNumber = sites$site[i]
# Download data from site
discdata = readNWISdv(siteNumber, parameterCd, startDate, endDate, statCd = statCd)
# Check if tempdata is empty
if (dim(discdata)[1] != 0){
# Conver to zoo format
discdata.zoo = zoo(discdata[,4], order.by = discdata[,3])
# Merge with dummy zoo time series to align dates
discmerge = merge(date.zoo, discdata.zoo, all= c(TRUE,TRUE), fill = NA)
# Store data in disc.df
disc.df[,(i+2)] = discmerge$discdata.zoo
}
}
View(disc.df)
disch.df = disc.df
## Calculate monthly medians of flow
# Reformat dates
disch.df$Date = as.Date(disch.df$Date, format="%m/%d/%Y")
# Add month and year columns
disch.df$month = month(disch.df$Date)
disch.df$yr = year(disch.df$Date)
View(disch.df)
View(disch.df)
View(disch.df)
# Aggregate discharge by month
disch.month = aggregate(disch.df[,c(-1,-2,-413,-414)],by=list(disch.df$month),FUN=median,na.rm=TRUE)
View(disch.month)
disch.month$Group.1 = c("JanQ","FebQ","MarQ", "AprQ", "MayQ", "JunQ", "JulQ","AugQ","SeptQ", "OctQ", "NovQ", "DecQ")
names(disch.month)[1] = "Month"
# Transpose dataframe
disch.month = as.data.frame(t(disch.month))
names(disch.month) = disch.month[1,]
disch.month = disch.month[-1,]
disch.month = cbind(rownames(disch.month), disch.month)
names(disch.month)[1] = "Site"
rownames(disch.month) = NULL
View(disch.month)
View(disch.month)
write.csv(disch.month,'E:/Users/Jeff/Github/stream-temperature-rf/data_download/Discharge/MedianQMonth_2016_2020.csv',row.names=FALSE)
library(stringi)
library(lubridate)
library(dplyr)
# Read in stream temperature daily mean values
stmean <- read.csv('E:/Users/Jeff/Github/stream-temperature-rf/data_download/StreamTemperature/ST_2016_2020.csv')
# Convert date column to date
stmean$Date <- as.Date(stmean$Date,format = "%m/%d/%Y")
# Fix stmean MonYr (insert missing 0 values)
stmean$MonYr <- as.character(stmean$MonYr)
View(stmean)
# Insert missing 0 values for months
for (i in seq(1,length(stmean$MonYr))){
if (nchar(stmean$MonYr[i]) == 5) {
stri_sub(stmean$MonYr[i],5,4) <-  0
}
}
# Convert MonYr values to integer
stmean$MonYr <- as.integer(stmean$MonYr)
# Calculate monthly means of stream temperature
stmax <- stmean %>% group_by(MonYr) %>% summarize_all(max, na.rm = TRUE)
warnings()
View(stmax)
# Replace Inf or -Inf with NA
stmax <- do.call(data.frame,
lapply(stmax,function(x) replace(x, is.infinite(x), NA)))
# Remove Date
stmax <- stmax[,c(-2)]
# Convert to Dataframe
stmax <- as.data.frame(t(stmax))
colnames(stmax) <- paste("M",stmax[1,],sep="")
stmax <- stmax[-1,]
# Move site names from rownames to column
stmax$Site <- colnames(stmean)[3:414]
stmax <- stmax[,c(49,1:48)]
rownames(stmax) <- NULL
View(stmax)
## CHANGE FILE DIRECTORY
# Read in stream temperature daily mean values
stmean <- read.csv('E:/Users/Jeff/Github/stream-temperature-rf/data_download/StreamTemperature/ST_2016_2020.csv')
# Convert date column to date
stmean$Date <- as.Date(stmean$Date,format = "%m/%d/%Y")
# Fix stmean MonYr values
stmean$MonYr <- as.character(stmean$MonYr)
# Insert missing 0 values for months
for (i in seq(1,length(stmean$MonYr))){
if (nchar(stmean$MonYr[i]) == 5) {
stri_sub(stmean$MonYr[i],5,4) <-  0
}
}
# Convert MonYr values to integer
stmean$MonYr <- as.integer(stmean$MonYr)
# Calculate monthly means of stream temperature
stmax <- stmean %>% group_by(MonYr) %>% summarize_all(max, na.rm = TRUE)
# Replace Inf or -Inf with NA
stmax <- do.call(data.frame,
lapply(stmax,function(x) replace(x, is.infinite(x), NA)))
# Remove Date
stmax <- stmax[,c(-2)]
# Convert to Dataframe
stmax <- as.data.frame(t(stmax))
colnames(stmax) <- paste("M",stmax[1,],sep="")
stmax <- stmax[-1,]
# Move site names from rownames to column
stmax$Site <- colnames(stmean)[3:412]
stmax <- stmax[,c(49,1:48)]
rownames(stmax) <- NULL
View(stmax)
# Aggregate 4 years of monthly means into mean monthly values
stmax_allmon <- data.frame(Site = stmax$Site,
Oct_max = rep(0,412), Nov_max = rep(0,412), Dec_max = rep(0,412),
Jan_max = rep(0,412), Feb_max = rep(0,412), Mar_max = rep(0,412),
Apr_max = rep(0,412), May_max = rep(0,412), Jun_max = rep(0,412),
Jul_max = rep(0,412), Aug_max = rep(0,412), Sep_max = rep(0,412))
# Aggregate 4 years of monthly means into mean monthly values
stmax_allmon <- data.frame(Site = stmax$Site,
Oct_max = rep(0,410), Nov_max = rep(0,410), Dec_max = rep(0,410),
Jan_max = rep(0,410), Feb_max = rep(0,410), Mar_max = rep(0,410),
Apr_max = rep(0,410), May_max = rep(0,410), Jun_max = rep(0,410),
Jul_max = rep(0,410), Aug_max = rep(0,410), Sep_max = rep(0,410))
View(stmax_allmon)
# loop through all months, calculating mean maximum st for each month across 4 years
for (i in seq(2,13)){
# Index columns for each month (i=3 is first october, by 12)
stmon <- stmax[,(seq(i,49,by=12))]
# Calculate mean for each month and assign to dataframe
stmax_allmon[,i] <- rowMeans(stmon, na.rm = TRUE)
}
View(stmax_allmon)
# Write to csv
write.csv(stmax_allmon,'E:/Users/Jeff/Github/stream-temperature-rf/metric_calculation/STmax.csv',row.names=FALSE)
